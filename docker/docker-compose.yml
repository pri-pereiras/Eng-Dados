version: '3.8'

services:
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: airflow
      MYSQL_DATABASE: airflow
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
    ports:
      - "3306:3306"
    networks:
      - laboratorio-airflow-tier

  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    networks:
      - laboratorio-airflow-tier

############################
##  DATA LAKE
############################
  minio:
    image: minio/minio:latest
    container_name: minio
    entrypoint: sh
    command:   '-c ''mkdir -p /minio_data/raw && mkdir -p /minio_data/trusted && mkdir -p /minio_data/refined && minio server /minio_data --console-address ":9001"'''
    ports:
      - "9050:9000"
      - "9051:9001"
    hostname: minio
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_ACCESS_KEY: datalake
      MINIO_SECRET_KEY: datalake
    volumes:
      - ./minio/data1:/data

  webserver:
    build: .
#    image: apache/airflow:2.7.2-python3.11
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2:// :airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: '${FERNET_KEY}2'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'True'
    volumes:
      - ../dags:/opt/airflow/dags
      - ../requirements.txt:/requirements.txt
      - C:/Users/Pedro/Documents/soutag_serv_repo/Novapasta:/opt/airflow/data
    ports:
      - "8080:8080"
    depends_on:
      - postgres
    networks:
      - laboratorio-airflow-tier
    command: >
      bash -c "pip install -r /requirements.txt && airflow db init && airflow webserver"

  scheduler:
    build: .
#    image: apache/airflow:2.7.2-python3.11
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: '${FERNET_KEY}2'
    volumes:
      - ../dags:/opt/airflow/dags
      - ../requirements.txt:/requirements.txt
      - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data
      - ./spark/jars/postgresql-42.5.1.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/postgresql-42.5.1.jar
      - ./spark/jars/hadoop-aws-3.2.4.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/hadoop-aws-3.2.4.jar
      - ./spark/jars/delta-core_2.12-2.1.0.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/delta-core_2.12-2.1.0.jar
      - ./spark/jars/delta-storage-2.1.0.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/delta-storage-2.1.0.jar
      - ./spark/jars/s3-2.18.41.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/s3-2.18.41.jar
      - ./spark/jars/guava-30.0-jre.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/guava-30.0-jre.jar
      - ./spark/jars/aws-java-sdk-bundle-1.11.901.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-bundle-1.11.901.jar
      - ./spark/jars/aws-java-sdk-core-1.11.901.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-core-1.11.901.jar
      - ./spark/jars/aws-java-sdk-kms-1.11.901.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-kms-1.11.901.jar
      - ./spark/jars/aws-java-sdk-s3-1.11.901.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-s3-1.11.901.jar
      - ./spark/jars/aws-java-sdk-dynamodb-1.11.901.jar:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-dynamodb-1.11.901.jar
    depends_on:
      - postgres
    networks:
      - laboratorio-airflow-tier
    command: >
      bash -c "pip install -r /requirements.txt && airflow scheduler"

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    hostname: spark-master
    container_name: spark-master
    ports:
      - '18080:8080'
      - '14040:4040'
      - '7077:7077'
    volumes:
      - ./spark/jars/postgresql-42.5.1.jar:/spark/jars/postgresql-42.5.1.jar
      - ./spark/jars/hadoop-aws-3.2.4.jar:/spark/jars/hadoop-aws-3.2.4.jar
      - ./spark/jars/delta-core_2.12-2.1.0.jar:/spark/jars/delta-core_2.12-2.1.0.jar
      - ./spark/jars/delta-storage-2.1.0.jar:/spark/jars/delta-storage-2.1.0.jar
      - ./spark/jars/s3-2.18.41.jar:/spark/jars/s3-2.18.41.jar
      - ./spark/jars/guava-30.0-jre.jar:/spark/jars/guava-30.0-jre.jar
      - ./spark/jars/aws-java-sdk-bundle-1.11.901.jar:/spark/jars/aws-java-sdk-bundle-1.11.901.jar
      - ./spark/jars/aws-java-sdk-core-1.11.901.jar:/spark/jars/aws-java-sdk-core-1.11.901.jar
      - ./spark/jars/aws-java-sdk-kms-1.11.901.jar:/spark/jars/aws-java-sdk-kms-1.11.901.jar
      - ./spark/jars/aws-java-sdk-s3-1.11.901.jar:/spark/jars/aws-java-sdk-s3-1.11.901.jar
      - ./spark/jars/aws-java-sdk-dynamodb-1.11.901.jar:/spark/jars/aws-java-sdk-dynamodb-1.11.901.jar
      - ./spark/scripts:/spark/scripts
      - ./spark/conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    depends_on:
      - minio
    environment:
      INIT_DAEMON_STEP: setup_spark
      SPARK_WORKLOAD: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_LOCAL_IP: spark-master
    networks:
      - laboratorio-airflow-tier

  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    hostname: spark-worker-1
    container_name: spark-worker-1
    ports:
      - '18081:8081'
      - '27077:7077'
    volumes:
      - ./spark/jars/postgresql-42.5.1.jar:/spark/jars/postgresql-42.5.1.jar
      - ./spark/jars/hadoop-aws-3.2.4.jar:/spark/jars/hadoop-aws-3.2.4.jar
      - ./spark/jars/delta-core_2.12-2.1.0.jar:/spark/jars/delta-core_2.12-2.1.0.jar
      - ./spark/jars/delta-storage-2.1.0.jar:/spark/jars/delta-storage-2.1.0.jar
      - ./spark/jars/s3-2.18.41.jar:/spark/jars/s3-2.18.41.jar
      - ./spark/jars/guava-30.0-jre.jar:/spark/jars/guava-30.0-jre.jar
      - ./spark/jars/aws-java-sdk-bundle-1.11.901.jar:/spark/jars/aws-java-sdk-bundle-1.11.901.jar
      - ./spark/jars/aws-java-sdk-core-1.11.901.jar:/spark/jars/aws-java-sdk-core-1.11.901.jar
      - ./spark/jars/aws-java-sdk-kms-1.11.901.jar:/spark/jars/aws-java-sdk-kms-1.11.901.jar
      - ./spark/jars/aws-java-sdk-s3-1.11.901.jar:/spark/jars/aws-java-sdk-s3-1.11.901.jar
      - ./spark/jars/aws-java-sdk-dynamodb-1.11.901.jar:/spark/jars/aws-java-sdk-dynamodb-1.11.901.jar
      - ./spark/scripts:/spark/scripts
      - ./spark/conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    depends_on:
      - spark-master
      - minio
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    networks:
      - laboratorio-airflow-tier

  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3 # apache-spark:3.3.0 # bitnami/spark:3.3.0
    hostname: spark-worker-2
    container_name: spark-worker-2
    ports:
      - '28081:8081'
      - '37077:7077'
    volumes:
      - ./spark/jars/postgresql-42.5.1.jar:/spark/jars/postgresql-42.5.1.jar
      - ./spark/jars/hadoop-aws-3.2.4.jar:/spark/jars/hadoop-aws-3.2.4.jar
      - ./spark/jars/delta-core_2.12-2.1.0.jar:/spark/jars/delta-core_2.12-2.1.0.jar
      - ./spark/jars/delta-storage-2.1.0.jar:/spark/jars/delta-storage-2.1.0.jar
      - ./spark/jars/s3-2.18.41.jar:/spark/jars/s3-2.18.41.jar
      - ./spark/jars/guava-30.0-jre.jar:/spark/jars/guava-30.0-jre.jar
      - ./spark/jars/aws-java-sdk-bundle-1.11.901.jar:/spark/jars/aws-java-sdk-bundle-1.11.901.jar
      - ./spark/jars/aws-java-sdk-core-1.11.901.jar:/spark/jars/aws-java-sdk-core-1.11.901.jar
      - ./spark/jars/aws-java-sdk-kms-1.11.901.jar:/spark/jars/aws-java-sdk-kms-1.11.901.jar
      - ./spark/jars/aws-java-sdk-s3-1.11.901.jar:/spark/jars/aws-java-sdk-s3-1.11.901.jar
      - ./spark/jars/aws-java-sdk-dynamodb-1.11.901.jar:/spark/jars/aws-java-sdk-dynamodb-1.11.901.jar
      - ./spark/scripts:/spark/scripts
      - ./spark/conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    depends_on:
      - spark-master
      - minio
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    networks:
      - laboratorio-airflow-tier

networks:
  laboratorio-airflow-tier:
    driver: bridge

